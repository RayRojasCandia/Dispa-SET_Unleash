{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cce1e5-a0c7-4d8b-a923-da1c7224ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entsoe import EntsoePandasClient\n",
    "import pandas as pd\n",
    "\n",
    "client = EntsoePandasClient(api_key='61e5bbbb-7e80-4540-a471-bd993873aa74')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6aeac48d-9eeb-4a61-893c-fbcf8de1e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp('20220101', tz='Europe/Brussels')\n",
    "end = pd.Timestamp('20220102', tz='Europe/Brussels')\n",
    "country_code = 'FI'  # Belgium\n",
    "country_code_from = 'FR'  # France\n",
    "country_code_to = 'DE_LU' # Germany-Luxembourg\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc55ab86-4631-4bae-8aca-c4e78fbee843",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoMatchingDataError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[1;32mIn[39], line 1\u001b[0m\n    query_unavailability_of_generation_units = client.query_unavailability_of_generation_units(country_code, start=start, end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/entsoe.py:1981\u001b[0m in \u001b[1;35mquery_unavailability_of_generation_units\u001b[0m\n    df = self._query_unavailability(\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/entsoe/decorators.py:152\u001b[0;36m in \u001b[0;35myear_wrapper\u001b[0;36m\n\u001b[0;31m    raise NoMatchingDataError\u001b[0;36m\n",
      "\u001b[0;31mNoMatchingDataError\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query_unavailability_of_generation_units = client.query_unavailability_of_generation_units(country_code, start=start, end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a0440db-1de1-4a86-9a8b-59c988a5f5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avail_qty</th>\n",
       "      <th>biddingzone_domain</th>\n",
       "      <th>businesstype</th>\n",
       "      <th>curvetype</th>\n",
       "      <th>docstatus</th>\n",
       "      <th>end</th>\n",
       "      <th>mrid</th>\n",
       "      <th>nominal_power</th>\n",
       "      <th>plant_type</th>\n",
       "      <th>production_resource_id</th>\n",
       "      <th>production_resource_location</th>\n",
       "      <th>production_resource_name</th>\n",
       "      <th>production_resource_psr_name</th>\n",
       "      <th>pstn</th>\n",
       "      <th>qty_uom</th>\n",
       "      <th>resolution</th>\n",
       "      <th>revision</th>\n",
       "      <th>start</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_doc_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-11 18:34:09+01:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-01 00:00:00+01:00</td>\n",
       "      <td>uo3Zwdwb6RoECvqjwFIHTQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fossil Oil</td>\n",
       "      <td>17W100P100P0250M</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CORDEMAIS 3</td>\n",
       "      <td>CORDEMAIS 3</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT1M</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-04-01 00:00:00+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-25 17:32:45+02:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>2026-01-01 00:00:00+01:00</td>\n",
       "      <td>28-kW3SFMUbkmXhO3jjLqg</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>17W100P100P0352E</td>\n",
       "      <td>France</td>\n",
       "      <td>CYCOFOS TV2</td>\n",
       "      <td>CYCOFOS PL2</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT60M</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-29 00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19 09:45:23+01:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>2022-03-04 17:00:00+01:00</td>\n",
       "      <td>GaVxT4UIYiEVnuwA4j48Hw</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Hydro Pumped Storage</td>\n",
       "      <td>17W100P100P02780</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>REVIN</td>\n",
       "      <td>REVIN 4</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT60M</td>\n",
       "      <td>8</td>\n",
       "      <td>2020-04-01 07:00:00+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-26 09:19:10+02:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Unplanned outage</td>\n",
       "      <td>A03</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-01 00:00:00+01:00</td>\n",
       "      <td>SBBDlApI938XfEA0GwgaNQ</td>\n",
       "      <td>595.0</td>\n",
       "      <td>Fossil Hard coal</td>\n",
       "      <td>17W100P100P03469</td>\n",
       "      <td>France</td>\n",
       "      <td>PROVENCE 5</td>\n",
       "      <td>PROVENCE 5</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT15M</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01 00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-04 12:00:46+02:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>2022-01-02 17:00:00+01:00</td>\n",
       "      <td>bDYxG_SMXFTT852hhq0B2w</td>\n",
       "      <td>259.0</td>\n",
       "      <td>Hydro Pumped Storage</td>\n",
       "      <td>17W100P100P0273A</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>CHEYLAS</td>\n",
       "      <td>CHEYLAS 1</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT60M</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-02 08:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-21 16:10:34+01:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>2028-01-01 00:00:00+01:00</td>\n",
       "      <td>U7gjAqnlqcx4DkB_mZQDQg</td>\n",
       "      <td>580.0</td>\n",
       "      <td>Fossil Hard coal</td>\n",
       "      <td>17W100P100P0238C</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>HAVRE 4</td>\n",
       "      <td>HAVRE 4</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT60M</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-04-01 00:00:00+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-14 17:40:24+01:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-14 15:32:00+01:00</td>\n",
       "      <td>Ec4HaVVMzQ0oenD2L5bZ1w</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Hydro Pumped Storage</td>\n",
       "      <td>17W100P100P02780</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>REVIN</td>\n",
       "      <td>REVIN 4</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT1M</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-12-14 15:30:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-14 17:40:24+01:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-14 15:30:00+01:00</td>\n",
       "      <td>Ec4HaVVMzQ0oenD2L5bZ1w</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Hydro Pumped Storage</td>\n",
       "      <td>17W100P100P02780</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>REVIN</td>\n",
       "      <td>REVIN 4</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT30M</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-12-14 15:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-14 17:40:24+01:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-14 15:00:00+01:00</td>\n",
       "      <td>Ec4HaVVMzQ0oenD2L5bZ1w</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Hydro Pumped Storage</td>\n",
       "      <td>17W100P100P02780</td>\n",
       "      <td>FRANCE</td>\n",
       "      <td>REVIN</td>\n",
       "      <td>REVIN 4</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT60M</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-06-29 07:00:00+02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-15 17:37:53+02:00</th>\n",
       "      <td>0</td>\n",
       "      <td>FR</td>\n",
       "      <td>Planned maintenance</td>\n",
       "      <td>A03</td>\n",
       "      <td>None</td>\n",
       "      <td>2034-09-01 00:00:00+02:00</td>\n",
       "      <td>ldy2JgBnwJNVFWOj7eO60A</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>17W100P100P0352E</td>\n",
       "      <td>France</td>\n",
       "      <td>CYCOFOS TV2</td>\n",
       "      <td>CYCOFOS PL2</td>\n",
       "      <td>1</td>\n",
       "      <td>MAW</td>\n",
       "      <td>PT60M</td>\n",
       "      <td>192</td>\n",
       "      <td>2017-05-15 02:00:00+02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          avail_qty  ...                     start\n",
       "created_doc_time                     ...                          \n",
       "2017-12-11 18:34:09+01:00         0  ... 2018-04-01 00:00:00+02:00\n",
       "2019-10-25 17:32:45+02:00         0  ... 2019-10-29 00:00:00+01:00\n",
       "2020-03-19 09:45:23+01:00         0  ... 2020-04-01 07:00:00+02:00\n",
       "2021-04-26 09:19:10+02:00         0  ... 2022-01-01 00:00:00+01:00\n",
       "2021-09-04 12:00:46+02:00         0  ... 2022-01-02 08:00:00+01:00\n",
       "...                             ...  ...                       ...\n",
       "2023-11-21 16:10:34+01:00         0  ... 2021-04-01 00:00:00+02:00\n",
       "2023-12-14 17:40:24+01:00         0  ... 2023-12-14 15:30:00+01:00\n",
       "2023-12-14 17:40:24+01:00         0  ... 2023-12-14 15:00:00+01:00\n",
       "2023-12-14 17:40:24+01:00         0  ... 2020-06-29 07:00:00+02:00\n",
       "2024-05-15 17:37:53+02:00         0  ... 2017-05-15 02:00:00+02:00\n",
       "\n",
       "[72 rows x 18 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_unavailability_of_generation_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68912f-03f7-49e9-b748-d705ae7a4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_units = client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5e1df-e56c-4546-82f8-f22f60df2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22ab56-5a19-4243-bf92-52f749003eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_installed_generation_capacity_per_units = client.query_installed_generation_capacity_per_unit(country_code, start=start, end=end, psr_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a733d-0634-45dc-88e7-97390344bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_installed_generation_capacity_per_units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee01ab6-9fee-4abb-9c6b-d388798ad5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/home/ray/Desktop/Backup/Prueva/1.csv'\n",
    "query_installed_generation_capacity_per_units.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affc0a0-b5a3-4c41-9dea-f05b9dfae371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9bf5d-c5dc-4cbb-bd33-eb660c48b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_units = client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1873fe-f12f-46b0-9210-5978d30fbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac1cbd-de8a-4450-b074-9627ccc09234",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/home/ray/Desktop/Backup/Prueva/prueva1.csv'\n",
    "query_unavailability_of_generation_units.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00449b56-6e34-4c72-a8bc-246bc210bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = client.query_installed_generation_capacity_per_unit(country_code, start=start, end=end, psr_type=None)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd988b-767f-48fe-af25-ef9ef9073852",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path_1 = '/home/ray/Desktop/Backup/Prueva/prueva2.csv'\n",
    "data_1.to_csv(output_file_path_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b2a02-b197-4a9a-b2e8-4741cf5b068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = client.query_generation(country_code, start=start, end=end, psr_type=None)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c771d3e-8807-41c6-896b-4107b386af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "import csv\n",
    "\n",
    "# Set max_headers to a higher value\n",
    "http.client._MAXHEADERS = 1000\n",
    "\n",
    "# Function to download table data from a URL\n",
    "def download_table_data(url):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Raise an error if there are too many headers\n",
    "    if len(response.headers) > 100:\n",
    "        raise Exception(\"Too many headers in the response\")\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table element\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract data from the table\n",
    "    data = []\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        if row_data:  # Ensures we don't add empty rows\n",
    "            data.append(row_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://transparency.entsoe.eu/generation/r2/waterReservoirsAndHydroStoragePlants/show?name=&defaultValue=false&viewType=TABLE&areaType=CTY&atch=false&dateTime.dateTime=01.01.2016+00:00|UTC|YEAR&dateTime.endDateTime=01.01.2024+00:00|UTC|YEAR&area.values=CTY|10YAT-APG------L!CTY|10YAT-APG------L'\n",
    "table_data = download_table_data(url)\n",
    "if table_data:\n",
    "    for row in table_data:\n",
    "        print(row)\n",
    "\n",
    "    output_file_path = \"/home/ray/Desktop/Backup/Prueva/prueva1.csv\"\n",
    "    with open(output_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a0e90-4ce6-47fb-8df5-0e54ceffc3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76ba89-e1fa-406f-8171-af946e496d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a101d-b598-41e7-9556-294e919a8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the variables\n",
    "start_year = 2016\n",
    "end_year = 2024\n",
    "data_year = 2023\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Change the name of the original CSV file\n",
    "new_file_name = f\"{start_year}_{end_year}.csv\"\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), new_file_name)\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(new_file_path)\n",
    "\n",
    "# Extract the required columns\n",
    "columns_to_extract = ['Week', str(data_year)]  # Assuming the column with data_year as name is string type\n",
    "df_extracted = df[columns_to_extract]\n",
    "\n",
    "# Save the extracted columns to a new CSV file\n",
    "output_file_name = f\"{data_year}_1.csv\"\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), output_file_name)\n",
    "df_extracted.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4a62e-5ade-4f93-be7e-4921d72cd152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958f1e3-96f1-432a-ae3f-09b5b194dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the variables\n",
    "data_year = \"2023\"\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract week number from the 'Week' column\n",
    "df['Week'] = df['Week'].str.extract(r'(\\d+)').astype(int)  # Extract digits from the string and convert to int\n",
    "\n",
    "# Calculate the dates\n",
    "start_date = pd.to_datetime(f'{data_year}-01-01')  # Start from January 1st of the specified year\n",
    "df['Dispa-SET_Date'] = start_date + pd.to_timedelta((df['Week'] - 1) * 7, unit='D')  # Add the corresponding number of weeks\n",
    "\n",
    "# Convert to string in the desired format\n",
    "df['Dispa-SET_Date'] = df['Dispa-SET_Date'].dt.strftime('%Y-%m-%d 00:00:00+00:00')\n",
    "\n",
    "# Save the DataFrame back to the original CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Print the result\n",
    "print(\"DataFrame saved back to the original CSV file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac08ea-0d07-4d70-9e30-2d64bbfb4d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e6a23-b2e9-4572-9780-ac8d2afd300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the year\n",
    "year = 2020\n",
    "\n",
    "# Get the last day of the year\n",
    "last_day_of_year = datetime.date(year, 12, 31)\n",
    "\n",
    "# Get the ISO calendar week number of the last day of the year\n",
    "_, last_week_number, _ = last_day_of_year.isocalendar()\n",
    "\n",
    "print(f\"The number of weeks in {year} is {last_week_number}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86197d-8fe4-422a-8429-c6717b6b3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the start and end years\n",
    "start_year = 2016\n",
    "end_year = 2024\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/\"\n",
    "\n",
    "# Define the file name\n",
    "file_name = f\"{start_year}_{end_year}.csv\"\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new row with 'Week 54' in the first column\n",
    "df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
    "\n",
    "# Iterate over columns (excluding the first one)\n",
    "for col in df.columns[1:]:\n",
    "    # Read the last row of the column\n",
    "    last_row_value = df.iloc[-1][col]\n",
    "    \n",
    "    # If the last row is empty\n",
    "    if pd.isna(last_row_value):\n",
    "        # Copy the value from the next column\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_first_value = df.iloc[0][next_col_name]\n",
    "            df.iloc[-1, df.columns.get_loc(col)] = next_col_first_value\n",
    "        \n",
    "        # Repeat the same process for the penultimate row\n",
    "        penultimate_row_value = df.iloc[-2][col]\n",
    "        if pd.isna(penultimate_row_value):\n",
    "            if next_col_index < len(df.columns):\n",
    "                next_col_second_value = df.iloc[1][next_col_name]\n",
    "                df.iloc[-2, df.columns.get_loc(col)] = next_col_second_value\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Process completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089172cb-e470-4923-bbaf-b56da1ec925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory path and file name\n",
    "folder_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/\"\n",
    "file_name = f\"{start_year}_{end_year}.csv\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new row with 'Week 54' in the first column\n",
    "df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
    "\n",
    "# Iterate over the rest of the columns (from the second till the last)\n",
    "for col in df.columns[1:]:\n",
    "    # Read the penultimate and last rows of the column\n",
    "    penultimate_value = df.iloc[-2][col]\n",
    "    last_value = df.iloc[-1][col]\n",
    "\n",
    "    # If the penultimate row is empty\n",
    "    if pd.isna(penultimate_value):\n",
    "        # Copy the value from the next column to the penultimate field read\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_first_value = df.iloc[0][next_col_name]\n",
    "            df.iloc[-2, df.columns.get_loc(col)] = next_col_first_value\n",
    "\n",
    "    # Copy the value of the second field from the next column to the last field of the current column\n",
    "    if pd.isna(last_value):\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_second_value = df.iloc[1][next_col_name]\n",
    "            df.iloc[-1, df.columns.get_loc(col)] = next_col_second_value\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6685f-aa7a-494b-85d3-669663e839d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842af0be-dc8f-4c24-b9eb-ddcb057ccb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the last value of the first column from file_path_1\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "last_date_file1 = df1.iloc[-1, 0]\n",
    "\n",
    "# Read the last value of the Dispa-SET_Date column from file_path_2\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "last_date_file2 = df2['Dispa-SET_Date'].iloc[-1]\n",
    "\n",
    "# Check if last date from file_path_1 is less than last date from file_path_2\n",
    "if last_date_file1 < last_date_file2:\n",
    "    # Calculate hourly steps until reaching the last date from file_path_2\n",
    "    hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='h')\n",
    "\n",
    "    # Create a new DataFrame with hourly steps\n",
    "    df_new_rows = pd.DataFrame({'Unnamed: 0': hourly_steps})\n",
    "\n",
    "    # Append the new rows to file_path_1 DataFrame\n",
    "    df1 = pd.concat([df1, df_new_rows], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "    print(\"New rows added successfully.\")\n",
    "else:\n",
    "    print(\"No new rows need to be added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760a87c-6f58-458e-b03f-397e8984808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbadb7-7aac-4c42-a33d-7f3f66e3095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8b95c-6d5f-4919-8bb5-9494dffcaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path (replace with your actual path)\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5874760-2caa-40c7-ac4e-dbd68658d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeae031-f0b8-466a-a9a2-5451a93f344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the last field of the first column from file_path_1\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "last_date_file1 = pd.to_datetime(df1.iloc[-1, 0])\n",
    "\n",
    "# Read the last field of the 'Dispa-SET_Date' column from file_path_2\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "last_date_file2 = pd.to_datetime(df2['Dispa-SET_Date'].iloc[-1])\n",
    "\n",
    "# Check if last date from file_path_1 is less than last date from file_path_2\n",
    "if last_date_file1 < last_date_file2:\n",
    "    # Calculate hourly steps until reaching the last date from file_path_2\n",
    "    hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='1H')\n",
    "\n",
    "    # Create a new DataFrame with hourly steps\n",
    "    df_new_rows = pd.DataFrame({'Unnamed: 0': hourly_steps})\n",
    "\n",
    "    # Append the new rows to file_path_1 DataFrame\n",
    "    df1 = pd.concat([df1, df_new_rows], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "    print(f\"New rows added successfully to {file_path_1}\")\n",
    "else:\n",
    "    print(f\"No new rows need to be added to {file_path_1}\")\n",
    "\n",
    "print(\"Overall processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15561717-66c4-45aa-8e2e-7bc8e1a6e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Remove rows with duplicated values in the first column\n",
    "df = df.drop_duplicates(subset=df.columns[0], keep='first')\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(\"Duplicates removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c0470-ff88-499f-a456-636d16a58228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths and data year\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "data_year = 2023\n",
    "\n",
    "# Read CSV files\n",
    "df_1 = pd.read_csv(file_path_1)\n",
    "df_2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Get the column name corresponding to the data year\n",
    "year_column = str(data_year)\n",
    "\n",
    "# Get the minimum length of both DataFrames\n",
    "min_length = min(len(df_1), len(df_2))\n",
    "\n",
    "# Iterate over the rows of both DataFrames up to the minimum length\n",
    "for i in range(min_length):\n",
    "    # Get the value from the 'Dispa-SET_Date' column in df_2\n",
    "    date_value = df_2.at[i, 'Dispa-SET_Date']\n",
    "    # Check if the value exists in the first column of df_1\n",
    "    if date_value in df_1[df_1.columns[0]].values:\n",
    "        # Get the index where the value is found in df_1\n",
    "        index_values = df_1.index[df_1[df_1.columns[0]] == date_value]\n",
    "        # Iterate over the found index values\n",
    "        for index_value in index_values:\n",
    "            # Copy corresponding value from df_2 and paste it to the corresponding fields of the second and third columns in df_1\n",
    "            df_1.at[index_value, df_1.columns[1]] = df_2.at[i, year_column]\n",
    "            df_1.at[index_value, df_1.columns[2]] = df_2.at[i, year_column]\n",
    "    else:\n",
    "        print(f\"Date value {date_value} not found in file_path_1.\")\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df_1.to_csv(file_path_1, index=False)\n",
    "print(\"Values copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d8b6d-fefb-4f92-84b3-821d82dd8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = '/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Perform linear interpolation along the second and third columns\n",
    "df[['HPHS', 'HDAM']] = df[['HPHS', 'HDAM']].interpolate(method='linear')\n",
    "\n",
    "# Save the interpolated DataFrame back to the CSV file\n",
    "df.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(\"Linear interpolation completed for the second and third columns and saved to file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2bfa4-1ea1-4f0e-a703-29999abb16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/BG/1h/2023.csv\"\n",
    "\n",
    "# Define the data year\n",
    "data_year = 2023\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Convert the first column to datetime\n",
    "df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])\n",
    "\n",
    "# Extract the year from the date values\n",
    "df['Year'] = df['Unnamed: 0'].dt.year\n",
    "\n",
    "# Filter out rows that belong to the specified year\n",
    "df_filtered = df[df['Year'] == data_year].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "# Drop the 'Year' column\n",
    "df_filtered.drop(columns=['Year'], inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file\n",
    "df_filtered.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(f\"Rows not belonging to {data_year} have been removed from {file_path_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d460d9-c0c3-45c8-904a-b41fc023e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://web-api.tp.entsoe.eu/api?securityToken=61e5bbbb-7e80-4540-a471-bd993873aa74documentType=A71&processType=A33&psrType=B02&in_Domain=10YCZ-CEPS-----N&periodStart=201512312300&periodEnd=201612312300\"\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Print the response content\n",
    "    print(response.text)\n",
    "else:\n",
    "    # Print an error message if the request was not successful\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e851d-b2c3-4a01-9235-88d4eaabbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the paths for input Excel file and output CSV file\n",
    "input_excel_path = '/home/ray/Downloads/Unavailability of Production and Generation Units_202301010000-202401010000.xlsx'\n",
    "output_csv_path = '/home/ray/Desktop/Backup/Prueva/prueva1.csv'\n",
    "\n",
    "# Load the Excel file into a pandas DataFrame\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b505c73-ad77-458b-b6e4-d4b82cafb320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f4786-64f1-493e-8922-5242911a5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first 6 rows\n",
    "df = pd.read_csv('/home/ray/Desktop/Backup/Prueva/prueva1.csv', skiprows=6)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('/home/ray/Desktop/Backup/Prueva/prueva1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30672920-8d23-4825-ae55-281fbb6e80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fd6d8-01e1-40e3-b962-cda5a7e3d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'\n",
    "\n",
    "# Define the path to the outage factors raw data folder\n",
    "outage_factors_raw_data_folder_path = f'/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/{data_year}/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(outage_factors_raw_data_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize variable to store headers\n",
    "headers = None\n",
    "\n",
    "# Iterate over each country code\n",
    "for country_code in country_list:\n",
    "    # Define the start and end timestamps for the year\n",
    "    start = pd.Timestamp(str(data_year) + '0101', tz='Europe/Brussels')\n",
    "    end = pd.Timestamp(str(data_year) + '1231', tz='Europe/Brussels')\n",
    "\n",
    "    # Query unavailability of generation units for the current country code\n",
    "    query_unavailability_of_generation_units = client.query_unavailability_of_generation_units(country_code, start=start, end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)\n",
    "    \n",
    "    # Check if the resulting DataFrame is empty\n",
    "    if not query_unavailability_of_generation_units.empty:\n",
    "        # Define the output file path using the country code\n",
    "        output_file_path = f'{outage_factors_raw_data_folder_path}{country_code}.csv'\n",
    "        \n",
    "        # Save the queried data to a CSV file\n",
    "        query_unavailability_of_generation_units.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        # Update headers if needed\n",
    "        headers = query_unavailability_of_generation_units.columns\n",
    "    else:\n",
    "        # If the DataFrame is empty, create an empty CSV file with the same headers as the last file saved\n",
    "        if headers is not None:\n",
    "            empty_csv_path = f'{outage_factors_raw_data_folder_path}{country_code}.csv'\n",
    "            pd.DataFrame(columns=headers).to_csv(empty_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4534ce-9b52-44f3-b7c2-9053e0c99eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54ecaf43-4709-48c2-a41b-30f836feb490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12610/36057424.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = grouped.apply(check_overlapping).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the CSV file path\n",
    "file_path = \"/home/ray/Desktop/Backup/AT_1.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'start' and 'end' columns to datetime\n",
    "df['start'] = pd.to_datetime(df['start'])\n",
    "df['end'] = pd.to_datetime(df['end'])\n",
    "\n",
    "# Initialize the 'overlapping' column with 0\n",
    "df['overlapping'] = 0\n",
    "\n",
    "# Group by 'production_resource_name'\n",
    "grouped = df.groupby('production_resource_name')\n",
    "\n",
    "# Function to check overlapping\n",
    "def check_overlapping(group):\n",
    "    # Sort the group by 'start' time\n",
    "    group = group.sort_values('start').reset_index(drop=True)\n",
    "    # Iterate through the group to check overlapping\n",
    "    for i in range(len(group) - 1):\n",
    "        if group.loc[i, 'end'] > group.loc[i + 1, 'start']:\n",
    "            group.loc[i, 'overlapping'] = 1\n",
    "            group.loc[i + 1, 'overlapping'] = 1\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "df = grouped.apply(check_overlapping).reset_index(drop=True)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Task completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf514b4-1d48-4f1e-9169-790c88cac449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19affc03-eaa3-4753-85b2-89f932db9673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/home/ray/Desktop/Backup/AT_1.csv\"\n",
    "\n",
    "# Function to find and handle overlapping periods\n",
    "def handle_overlaps(df):\n",
    "    # Convert start and end columns to datetime\n",
    "    df['start'] = pd.to_datetime(df['start'])\n",
    "    df['end'] = pd.to_datetime(df['end'])\n",
    "\n",
    "    # Sort the dataframe by production_resource_name and start time\n",
    "    df = df.sort_values(by=['production_resource_name', 'start'])\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each group of production_resource_name\n",
    "    for name, group in df.groupby('production_resource_name'):\n",
    "        current_period = None\n",
    "        for _, row in group.iterrows():\n",
    "            if current_period is None:\n",
    "                current_period = row\n",
    "            else:\n",
    "                if row['start'] <= current_period['end']:\n",
    "                    # Overlapping period, merge it\n",
    "                    current_period['end'] = max(current_period['end'], row['end'])\n",
    "                    current_period['avail_qty'] += row['avail_qty']\n",
    "                    current_period['nominal_power'] += row['nominal_power']\n",
    "                else:\n",
    "                    # No overlap, append the current period and move to the next\n",
    "                    new_rows.append(current_period)\n",
    "                    current_period = row\n",
    "        if current_period is not None:\n",
    "            new_rows.append(current_period)\n",
    "\n",
    "    # Create a new DataFrame from the new_rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Process the dataframe to handle overlaps\n",
    "df_result = handle_overlaps(df)\n",
    "\n",
    "# Save the result back to the CSV file\n",
    "df_result.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbaefe-c61b-474b-96a3-f8282f07ac37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9aca5ba4-1dd0-4ffe-8eda-9f8c2b60dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"/home/ray/Desktop/Backup/AT_1.csv\"\n",
    "\n",
    "# Function to handle overlaps and create new rows\n",
    "def handle_overlaps(df):\n",
    "    # Convert start and end columns to datetime\n",
    "    df['start'] = pd.to_datetime(df['start'])\n",
    "    df['end'] = pd.to_datetime(df['end'])\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each group of production_resource_name\n",
    "    for name, group in df.groupby('production_resource_name'):\n",
    "        group = group.sort_values(by=['start'])\n",
    "        while not group.empty:\n",
    "            row = group.iloc[0]\n",
    "            group = group.iloc[1:]\n",
    "            \n",
    "            overlapping_rows = group[(group['start'] < row['end']) & (group['end'] > row['start'])]\n",
    "            if not overlapping_rows.empty:\n",
    "                overlap_start = max(row['start'], overlapping_rows['start'].min())\n",
    "                overlap_end = min(row['end'], overlapping_rows['end'].max())\n",
    "\n",
    "                overlap_avail_qty = row['avail_qty'] + overlapping_rows['avail_qty'].sum()\n",
    "                overlap_nominal_power = row['nominal_power'] + overlapping_rows['nominal_power'].sum()\n",
    "\n",
    "                new_rows.append({\n",
    "                    'avail_qty': overlap_avail_qty,\n",
    "                    'nominal_power': overlap_nominal_power,\n",
    "                    'production_resource_name': name,\n",
    "                    'start': overlap_start,\n",
    "                    'end': overlap_end\n",
    "                })\n",
    "\n",
    "                if row['start'] < overlap_start:\n",
    "                    new_rows.append({\n",
    "                        'avail_qty': row['avail_qty'],\n",
    "                        'nominal_power': row['nominal_power'],\n",
    "                        'production_resource_name': name,\n",
    "                        'start': row['start'],\n",
    "                        'end': overlap_start\n",
    "                    })\n",
    "\n",
    "                if row['end'] > overlap_end:\n",
    "                    new_rows.append({\n",
    "                        'avail_qty': row['avail_qty'],\n",
    "                        'nominal_power': row['nominal_power'],\n",
    "                        'production_resource_name': name,\n",
    "                        'start': overlap_end,\n",
    "                        'end': row['end']\n",
    "                    })\n",
    "\n",
    "                group = group[~((group['start'] < row['end']) & (group['end'] > row['start']))]\n",
    "\n",
    "            else:\n",
    "                new_rows.append(row.to_dict())\n",
    "\n",
    "    # Create a new DataFrame from the new_rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Process the dataframe to handle overlaps\n",
    "df_result = handle_overlaps(df)\n",
    "\n",
    "# Save the result back to the CSV file\n",
    "df_result.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Task completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56e6f0-5f8a-48d3-986f-ee5e6085ca88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2a66b-f30b-4c9f-b6f2-d1977ab9e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/AT_1.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/2023.csv\"\n",
    "\n",
    "# Load the CSV files into dataframes\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Create a new column 'Total_Capacity' in df1 and initialize it with NaN\n",
    "df1['Total_Capacity'] = pd.NA\n",
    "\n",
    "# Iterate over each row in df2\n",
    "for index, row in df2.iterrows():\n",
    "    unit_name = row['Unit']\n",
    "    power_capacity = row['PowerCapacity']\n",
    "    \n",
    "    # Find matching rows in df1\n",
    "    matching_rows = df1[df1['production_resource_name'] == unit_name]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        # Update the 'Total_Capacity' in the matching rows\n",
    "        df1.loc[matching_rows.index, 'Total_Capacity'] = power_capacity\n",
    "\n",
    "# Save the updated dataframe back to a CSV file\n",
    "df1.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(\"Update completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74ae60-64b1-46aa-adcd-91cb5ea43616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5e0d1-5bb1-4eee-8e79-2528a1a5b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_matching_column(file_path_1, file_path_2):\n",
    "  \"\"\"\n",
    "  Reads CSV files, finds matching production resources, and fills a column with '1'.\n",
    "\n",
    "  Args:\n",
    "      file_path_1 (str): Path to the first CSV file (AT_1.csv).\n",
    "      file_path_2 (str): Path to the second CSV file (2023.csv).\n",
    "\n",
    "  Returns:\n",
    "      None: Modifies the second CSV file in-place.\n",
    "  \"\"\"\n",
    "\n",
    "  # Read both CSV files using pandas\n",
    "  df1 = pd.read_csv(file_path_1)\n",
    "  df2 = pd.read_csv(file_path_2)\n",
    "\n",
    "  # Ensure 'production_resource_name' exists in file_path_1\n",
    "  if 'production_resource_name' not in df1.columns:\n",
    "    print(\"Warning: 'production_resource_name' column not found in file_path_1. Skipping processing.\")\n",
    "    return\n",
    "\n",
    "  # Iterate through unique resource names in df1\n",
    "  for resource_name in df1['production_resource_name'].unique():\n",
    "    # Check if matching resource name exists as a column header in df2\n",
    "    if resource_name in df2.columns:\n",
    "      matching_column = df2[resource_name]\n",
    "      # Fill the entire matching column with '1'\n",
    "      matching_column.fillna(1, inplace=True)\n",
    "\n",
    "  # Save the modified second CSV (optional)\n",
    "  # df2.to_csv(file_path_2, index=False)  # Uncomment to save changes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  file_path_1 = \"/home/ray/Desktop/Backup/AT_1.csv\"\n",
    "  file_path_2 = \"/home/ray/Desktop/Backup/2023.csv\"\n",
    "  fill_matching_column(file_path_1, file_path_2)\n",
    "  print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba94d4-d6fc-47e5-8321-6967365a23b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
