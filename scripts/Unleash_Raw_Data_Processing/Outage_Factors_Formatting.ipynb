{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae637724-855a-4790-a97b-d734ffd9a106",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    TIME SERIES DATA PROCESSING\n",
    "    <br>\n",
    "    OUTAGE FACTORS\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    Main Formatting Notebook\n",
    "</div>\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Each part of the following script was used to proccess the raw data for the Outage Factors Time Series Raw Data for all the european countries of the Dispa-SET_Unleash project.\n",
    "<br>\n",
    "Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    1. Notebook Set Up\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb71f838-06e5-4102-a2b3-8ea0a349c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "import numpy as np\n",
    "from entsoe import EntsoePandasClient\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24f933-f4b0-4965-8ccd-c6cc7fa953ec",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "    2. Dispa-SET_Unleash Folder Path\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory.\n",
    "</div>\n",
    "<div style=\"text-align: jusitfy; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9fd200-8d82-4a17-bb81-43fe5e092298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14876099-b52c-4401-aae6-ed4eb77eafcf",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "    3. Usefull Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Entering a value to all the variables which content are going to be used in some of the next stages of this script. \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Indicate the year of all data is referring to in the variable data_year.\n",
    "<br>\n",
    "The universal_standar_time variable is going to be used to download all the time series data in this horary zone. Additionally as each european country belongs a particular time sector the corresponding time series data related to its time sector are going to be downloaded as well but in a different file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5daf69c-1298-4cad-8c69-05f37279f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_year = 2016\n",
    "\n",
    "# Universal standad time:\n",
    "universal_standard_time = 'UTC'\n",
    "\n",
    "# Western European Time:\n",
    "western_european_time = 'WET_WEST'\n",
    "\n",
    "# Central European Time:\n",
    "central_european_time = 'CET_CEST'\n",
    "\n",
    "# Eastern European Time:\n",
    "eastern_european_time = 'EET_EST'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dacfb8-17a6-4bda-b897-dfb25d737695",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "4. Outage Factors Directories Definition\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Creating the folders that are going to content all the data realted to the Outage Factors Level time series.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Futher the downloaded raw data is going to be used to get the Outage Factors time series. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8cf4e1-c49f-4ac9-958d-0103993f9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/\n",
      "outage_factors_folder_path: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/\n",
      "outage_factors_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/2016\n",
      "availability_factors_data_folder_path: /home/ray/Dispa-SET_Unleash/Database/AvailabilityFactors/\n",
      "power_plants_data_folder_path: /home/ray/Dispa-SET_Unleash/Database/PowerPlants/\n"
     ]
    }
   ],
   "source": [
    "# Additional string to be appended\n",
    "additional_path = \"/RawData/OutageFactors/\"\n",
    "additional_path_1 = \"/RawData/OutageFactors/Raw_Data_Sources/\"\n",
    "additional_path_2 = \"/Database/AvailabilityFactors/\"\n",
    "additional_path_3 = \"/Database/PowerPlants/\"\n",
    "\n",
    "# Construct the standard_time_data_folder_path variable\n",
    "reference_data_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Outage_Factors_folder_path variable\n",
    "outage_factors_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Outage_Factors_Raw_Data_folder_path variable\n",
    "outage_factors_raw_data_folder_path = dispaSET_unleash_folder_path + additional_path_1 + str(data_year)\n",
    "\n",
    "# Construct the Availability_Factors_Data_folder_path variable\n",
    "availability_factors_data_folder_path = dispaSET_unleash_folder_path + additional_path_2\n",
    "\n",
    "# Construct the Power_Plants_Data_folder_path variable\n",
    "power_plants_data_folder_path = dispaSET_unleash_folder_path + additional_path_3\n",
    "\n",
    "\n",
    "print(\"reference_data_folder_path:\", reference_data_folder_path)\n",
    "print(\"outage_factors_folder_path:\", outage_factors_folder_path)\n",
    "print(\"outage_factors_raw_data_folder_path:\", outage_factors_raw_data_folder_path)\n",
    "print(\"availability_factors_data_folder_path:\", availability_factors_data_folder_path)\n",
    "print(\"power_plants_data_folder_path:\", power_plants_data_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d4b76-5e53-4574-8270-f7a7c71315d6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    4.1. Back Up Directory\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    Saving the original files into a Back up folder.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Since in the next steps of the processing data new subfolders and files are going to be created, the original ones are saved in a back up foldet to return them as its default content ones the process will be finished.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f835e2-518e-46ce-8dd7-b07ae1fa7289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup_folder_path: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors_backup/\n",
      "Backup created at /home/ray/Dispa-SET_Unleash/RawData/OutageFactors_backup/\n"
     ]
    }
   ],
   "source": [
    "additional_path_5 = '/RawData/OutageFactors_backup/'\n",
    "\n",
    "# Construct the backup_folder_path variable\n",
    "backup_folder_path = dispaSET_unleash_folder_path + additional_path_5\n",
    "\n",
    "print(\"backup_folder_path:\", backup_folder_path)\n",
    "\n",
    "# Create a backup of the directory\n",
    "if os.path.exists(backup_folder_path):\n",
    "    shutil.rmtree(backup_folder_path)  # Remove any existing backup if necessary\n",
    "shutil.copytree(outage_factors_folder_path, backup_folder_path)\n",
    "\n",
    "print(f\"Backup created at {backup_folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceca83-46f2-48c1-920f-96f2ed7c08ae",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "4. European Standard Time\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "All the time series are going to be downloaded under the UTC <em>(The Worldâ€™s Time Standard)</em>.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "This is going to be done for each country as well.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "All this features are going to be saved in a csv file called Reference_Data.csv where all additional characters to the download process are going to be written.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cb6359-1534-43a0-bc52-d2c181861c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of countries and standard times\n",
    "countries = [\n",
    "    \"Austria\", \"Belgium\", \"Bulgaria\", \"Switzerland\", \"Cyprus\", \"Czech Republic\",\n",
    "    \"Germany\", \"Denmark\", \"Estonia\", \"Greece\", \"Spain\", \"Finland\", \"France\",\n",
    "    \"Croatia\", \"Hungary\", \"Ireland\", \"Italy\", \"Lithuania\", \"Luxembourg\", \"Latvia\",\n",
    "    \"Malta\", \"Netherlands\", \"Norway\", \"Poland\", \"Portugal\", \"Romania\", \"Sweden\",\n",
    "    \"Slovenia\", \"Slovakia\", \"United Kingdom\"\n",
    "]\n",
    "\n",
    "dispaSET_codes = [\"AT\", \"BE\", \"BG\", \"CH\", \"CY\", \"CZ\", \"DE\", \"DK\", \"EE\", \"EL\", \"ES\", \"FI\", \"FR\", \"HR\", \"HU\", \n",
    "                  \"IE\", \"IT\", \"LT\", \"LU\", \"LV\", \"MT\", \"NL\", \"NO\", \"PL\", \"PT\", \"RO\", \"SE\", \"SI\", \"SK\", \"UK\"\n",
    "]\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Country': countries, 'Dispa-SET_Code': dispaSET_codes})\n",
    "\n",
    "reference_data_file_name = 'Reference_Data.csv'\n",
    "\n",
    "# Construct the full file path\n",
    "reference_data_file_path = os.path.join(reference_data_folder_path, reference_data_file_name)\n",
    "\n",
    "# Create the CSV file with the specified name\n",
    "with open(reference_data_file_path, 'w') as f:\n",
    "    # Optional: Write a header if needed\n",
    "    # f.write(\"header1,header2,header3\\n\")\n",
    "\n",
    "# Save DataFrame to the CSV file\n",
    "    df.to_csv(reference_data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2604ea4-c766-4ddb-b287-580fa9aae588",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "819d5e77-fa29-4b7d-a224-63d35d32c32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name:                              Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path:                              /home/ray/Dispa-SET_Unleash\n",
      "data_year:                                                 2016\n",
      "universal_standard_time:                                   UTC\n",
      "western_european_time:                                     WET_WEST\n",
      "central_european_time:                                     CET_CEST\n",
      "eastern_european_time:                                     EET_EST\n",
      "reference_data_folder_path:                                /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/\n",
      "reference_data_file_name:                                  Reference_Data.csv\n",
      "reference_data_file_path:                                  /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Reference_Data.csv\n",
      "outage_factors_folder_path:                                /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/\n",
      "outage_factors_raw_data_folder_path:                        /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/2016\n",
      "power_plants_data_folder_path:                              /home/ray/Dispa-SET_Unleash/Database/PowerPlants/\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"universal_standard_time:                                   {universal_standard_time}\")\n",
    "print (f\"western_european_time:                                     {western_european_time}\")\n",
    "print (f\"central_european_time:                                     {central_european_time}\")\n",
    "print (f\"eastern_european_time:                                     {eastern_european_time}\")\n",
    "print (f\"reference_data_folder_path:                                {reference_data_folder_path}\")\n",
    "print (f\"reference_data_file_name:                                  {reference_data_file_name}\")\n",
    "print (f\"reference_data_file_path:                                  {reference_data_file_path}\")\n",
    "print (f\"outage_factors_folder_path:                                {outage_factors_folder_path}\")\n",
    "print(f\"outage_factors_raw_data_folder_path:                        {outage_factors_raw_data_folder_path}\")\n",
    "print(f\"power_plants_data_folder_path:                              {power_plants_data_folder_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39cd66f6-9fb1-4985-95f4-fb6fbc5dfcd3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5. Main Data Source\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Sellecting the main source to get the raw data of the time series.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "All the data to be processed was downloaded from one main source:\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 4.0em; font-weight: unbold; font-size: 12px; font-family: TimesNewRoman;color:skyblue\">\n",
    "<em><strong>ENTSOE Transparency Platform:</strong></em>\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "    Which the main url link is the following\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 4.0em; font-weight: unbold; font-size: 12px; font-family: TimesNewRoman;color:skyblue\">\n",
    "https://transparency.entsoe.eu/dashboard/show\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: jusitfy; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.1. Raw Data Sources Path\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Entering all the files paths where the raw data is content.\n",
    "     <br>\n",
    "        All the csv files with the outages raw data are storaged a folder with the name of the corresponding year in a local folder which path is:\n",
    "    <div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "/Local/Dispa_Set/Path/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/\n",
    "        </div>\n",
    "The path of this csv files are going to be added to the references data csv file for manipulation purposes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc16b795-e270-4c85-b34e-d10dd38492bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "#reference_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Reference_Data.csv\"\n",
    "#outage_factors_raw_data_folder_path = \"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources\"\n",
    "\n",
    "# Read the reference data CSV file\n",
    "reference_data = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Function to get file path from Dispa-SET code\n",
    "def get_file_path(code):\n",
    "    file_name = code + \".csv\"\n",
    "    file_path = os.path.join(outage_factors_raw_data_folder_path, file_name)\n",
    "    return file_path if os.path.exists(file_path) else None\n",
    "\n",
    "# Add a new column 'Raw_Data_File_Path' and fill it\n",
    "reference_data['Raw_Data_File_Path'] = reference_data['Dispa-SET_Code'].apply(get_file_path)\n",
    "\n",
    "# Save the updated dataframe back to the CSV file\n",
    "reference_data.to_csv(reference_data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827473f3-e6e0-4f4d-aee6-574c16c44b0c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.2. Unnecessary Raw Data \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Eliminating those rows and columns of the raw data files that content unrerquired and repeated data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8277c68-195e-4479-b389-223209add844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/2016/AT_1.csv\n",
      "Processed and saved: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/2016/BE_1.csv\n",
      "Processed and saved: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/2016/BG_1.csv\n",
      "Processed and saved: /home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/2016/CH_1.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'docstatus'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m in \u001b[1;35mget_loc\u001b[0m\n    return self._engine.get_loc(casted_key)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mindex.pyx:167\u001b[0m in \u001b[1;35mpandas._libs.index.IndexEngine.get_loc\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mindex.pyx:196\u001b[0m in \u001b[1;35mpandas._libs.index.IndexEngine.get_loc\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m in \u001b[1;35mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0;36m in \u001b[0;35mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;36m\n",
      "\u001b[0;31mKeyError\u001b[0m\u001b[0;31m:\u001b[0m 'docstatus'\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[1;32mIn[9], line 18\u001b[0m\n    df = df[df['docstatus'] != 'Cancelled']\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m in \u001b[1;35m__getitem__\u001b[0m\n    indexer = self.columns.get_loc(key)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0;36m in \u001b[0;35mget_loc\u001b[0;36m\n\u001b[0;31m    raise KeyError(key) from err\u001b[0;36m\n",
      "\u001b[0;31mKeyError\u001b[0m\u001b[0;31m:\u001b[0m 'docstatus'\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the reference data CSV file\n",
    "#reference_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Reference_Data.csv\"\n",
    "\n",
    "# Read the reference data CSV file into a DataFrame\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column to process which contains file paths to other CSV files\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for index, file_path in df_reference[column_to_process].dropna().items():\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Remove rows where the docstatus column has the value 'Cancelled'\n",
    "        df = df[df['docstatus'] != 'Cancelled']\n",
    "        \n",
    "        # Construct the new file path with '_1' suffix\n",
    "        base, ext = os.path.splitext(file_path)\n",
    "        new_file_path = f\"{base}_1{ext}\"\n",
    "        \n",
    "        # Save the modified CSV file to the new file path\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "        print(f\"Processed and saved: {new_file_path}\")\n",
    "        \n",
    "        # Update the path in the reference DataFrame\n",
    "        df_reference.at[index, column_to_process] = new_file_path\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Save the updated reference DataFrame back to the reference data file\n",
    "df_reference.to_csv(reference_data_file_path, index=False)\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215a44a-4216-4a8e-bc48-b918cd5acfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column containing file paths to process\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Define the columns to keep in the corresponding CSV files\n",
    "columns_to_keep = ['avail_qty', 'end', 'nominal_power', 'production_resource_name', 'start']\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for file_path in df_reference[column_to_process].dropna():\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if all specified columns exist in the DataFrame\n",
    "        if all(column in df.columns for column in columns_to_keep):\n",
    "            # Keep only the specified columns\n",
    "            df = df[columns_to_keep]\n",
    "            \n",
    "            # Save the modified CSV file back to its original location\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Processed and saved: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Columns not found in file {file_path}. Skipping.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddc602-72b6-4c6f-a5a8-fdf48c734183",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.3. UTC Time Convertion\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Changing the dates to the UTC time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26bac1-6ae8-4145-a064-d1255339f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference data CSV file path\n",
    "#reference_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Reference_Data.csv\"\n",
    "\n",
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column to process\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Function to convert a timestamp to UTC\n",
    "def convert_to_utc(timestamp_str):\n",
    "    local_time = datetime.fromisoformat(timestamp_str)\n",
    "    utc_time = local_time.astimezone(pytz.utc)\n",
    "    return utc_time.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Function to add a colon at the third position from the end\n",
    "def add_colon_third_from_end(timestamp_str):\n",
    "    return timestamp_str[:-2] + ':' + timestamp_str[-2:]\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for file_path in df_reference[column_to_process].dropna():\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the 'start' and 'end' columns to UTC\n",
    "        df['start'] = df['start'].apply(convert_to_utc)\n",
    "        df['end'] = df['end'].apply(convert_to_utc)\n",
    "        \n",
    "        # Add a colon at the third position from the end\n",
    "        df['start'] = df['start'].apply(add_colon_third_from_end)\n",
    "        df['end'] = df['end'].apply(add_colon_third_from_end)\n",
    "        \n",
    "        # Save the modified CSV file back to its original location\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Processed and saved: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ded09-55f1-474e-b340-a444d828d1e7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: Justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.4. Yearly Raw Data\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Sellecting the raw data inside the analized period of time (defined by the data_year variable).\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Some availabilities are given in dates outside the analyzed time period. e.g. started in a previous year or ended in a year after.\n",
    "Just those periods that are inside the analyzed year are going to be kept. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081b0e8-d770-4468-bc92-fb86f1e24dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column containing the file paths\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Function to update timestamps based on data year\n",
    "def update_timestamps(timestamp_str):\n",
    "    year_part = str(data_year)\n",
    "    start_of_year = f\"{year_part}-01-01 00:00:00+00:00\"\n",
    "    end_of_year = f\"{year_part}-12-31 23:59:00+00:00\"\n",
    "    \n",
    "    # Check if the timestamp is outside the data year range\n",
    "    if not (start_of_year <= timestamp_str <= end_of_year):\n",
    "        return start_of_year if timestamp_str < start_of_year else end_of_year\n",
    "    \n",
    "    # If the timestamp is within the data year range, keep it unchanged\n",
    "    return timestamp_str\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for file_path in df_reference[column_to_process].dropna():\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the expected columns exist\n",
    "    if 'start' not in df.columns or 'end' not in df.columns:\n",
    "        print(f\"Error: Missing columns in file - {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Update the 'start' and 'end' columns\n",
    "    df['start'] = df['start'].apply(update_timestamps)\n",
    "    df['end'] = df['end'].apply(update_timestamps)\n",
    "    \n",
    "    # Save the modified CSV file back to its original location\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Processed and saved: {file_path}\")\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26cd6a-c2cd-4dc9-9a68-218e53e267e6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.5. Inexistent Periods Raw Data\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Erasing those periors with the same start and end time .\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Due to some outage periords woul be shorter than the minimal time stept i.e. 15 minutes, some values may appear with the same value in is start and end time. \n",
    "<br>\n",
    "Those ones are deported from the raw data files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbdb93-3d90-40d0-bf9a-e16af79f5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference data CSV file path\n",
    "#reference_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Reference_Data.csv\"\n",
    "\n",
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column containing the file paths\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Function to remove rows with same 'start' and 'end' values\n",
    "def remove_same_start_end_rows(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the expected columns exist\n",
    "    if 'start' not in df.columns or 'end' not in df.columns:\n",
    "        print(f\"Error: Missing columns in file - {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Remove rows where 'start' and 'end' values are the same\n",
    "    df = df[df['start'] != df['end']]\n",
    "    \n",
    "    # Save the modified CSV file back to its original location\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Processed and saved: {file_path}\")\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for file_path in df_reference[column_to_process].dropna():\n",
    "    remove_same_start_end_rows(file_path)\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa45a8-b75e-4059-8836-3e6e49b4e24d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.6. Time Step Raw Data\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Addapting the data into the minimal time stept.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The time series like the load data and the production of the units were got in a minimal time speo of 15 minutes e.g.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 4.0em; font-weight: unbold; font-size: 12px; font-family: TimesNewRoman;color:skyblue\">\n",
    "2023-01-01 00:15:00+00:00\n",
    "<br>\n",
    "2023-01-01 00:30:00+00:00\n",
    "<br>\n",
    "2023-01-01 00:45:00+00:00\n",
    "<br>\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Since some of the outages from the sources are given in a diferent time stept e.g.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 4.0em; font-weight: unbold; font-size: 12px; font-family: TimesNewRoman;color:skyblue\">\n",
    "2023-01-01 00:37:00+00:00\n",
    "<br>\n",
    "2023-01-01 00:48:00+00:00\n",
    "<br>\n",
    "2023-01-01 00:52:00+00:00\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Those are going to be dadapted to the most closest value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81159120-0d38-40ad-8f5a-e7b968090cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round minutes to the nearest 15-minute interval\n",
    "def round_to_nearest_15_minutes(timestamp_str):\n",
    "    timestamp = datetime.fromisoformat(timestamp_str)\n",
    "    minute = timestamp.minute\n",
    "    \n",
    "    if minute < 8:\n",
    "        rounded_minutes = 0\n",
    "    elif minute < 23:\n",
    "        rounded_minutes = 15\n",
    "    elif minute < 38:\n",
    "        rounded_minutes = 30\n",
    "    elif minute < 53:\n",
    "        rounded_minutes = 45\n",
    "    else:\n",
    "        rounded_minutes = 0\n",
    "        timestamp += timedelta(hours=1)  # Increment the hour\n",
    "\n",
    "    rounded_timestamp = timestamp.replace(minute=rounded_minutes, second=0, microsecond=0)\n",
    "    return rounded_timestamp.isoformat()\n",
    "\n",
    "# Function to process a CSV file\n",
    "def process_csv_file(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Round the 'start' and 'end' columns to the nearest 15 minutes\n",
    "    df['start'] = df['start'].apply(round_to_nearest_15_minutes)\n",
    "    df['end'] = df['end'].apply(round_to_nearest_15_minutes)\n",
    "    \n",
    "    # Save the modified CSV file back to its original location\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Processed and saved: {file_path}\")\n",
    "\n",
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column to process\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for file_path in df_reference[column_to_process].dropna():\n",
    "    process_csv_file(file_path)\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611c64b-a293-44b2-88a3-797cd99c6e88",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.7. Duplicated Time Periods Raw Data \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Cleaning the outages that happened in the same time period.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Since each power plant can have more than one generator, the download data makes a sub-division accordingly, however, as the power plants data base are defined by a total power capacity, it is needed to set the outage data in the same way i.e. sum all the generators of the same power unit which outage time period is the same.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407be64e-1e06-4ba6-b86a-aceb280379bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the column to process\n",
    "column_to_process = 'Raw_Data_File_Path'\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv_file(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Group by the columns 'start', 'end', and 'production_resource_name'\n",
    "    grouped = df.groupby(['start', 'end', 'production_resource_name'], as_index=False)\n",
    "\n",
    "    # Sum the 'avail_qty' and 'nominal_power' columns\n",
    "    df_summed = grouped.agg({\n",
    "        'avail_qty': 'sum',\n",
    "        'nominal_power': 'sum'\n",
    "    })\n",
    "\n",
    "    # Merge the summed columns back with the non-summed columns\n",
    "    df_non_summed = grouped.head(1).drop(columns=['avail_qty', 'nominal_power'])\n",
    "    df_result = pd.merge(df_non_summed, df_summed, on=['start', 'end', 'production_resource_name'])\n",
    "\n",
    "    # Save the modified DataFrame back to a CSV file\n",
    "    df_result.to_csv(file_path, index=False)\n",
    "\n",
    "# Iterate over each file path in the specified column\n",
    "for file_path in df_reference[column_to_process].dropna():\n",
    "    process_csv_file(file_path)\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dd84d-da71-4347-9d0b-a496111077f2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "5.8. Overlapped Outage Time Periods \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Separating those outages that has overlapped time periods.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Since each power plant can have more than one generator, the download data makes a sub-division accordingly, in consequence the outages periods of these generators are specified individually in the raw data.\n",
    "<br>\n",
    "This in some cases leads to the presence of overlapping time periods that has to be separated.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d0a31-7cfb-47d1-9e4a-472a88fb788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference CSV file\n",
    "reference_df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Function to handle overlaps and create new rows\n",
    "def handle_overlaps(df):\n",
    "    # Convert start and end columns to datetime\n",
    "    df['start'] = pd.to_datetime(df['start'])\n",
    "    df['end'] = pd.to_datetime(df['end'])\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate over each group of production_resource_name\n",
    "    for name, group in df.groupby('production_resource_name'):\n",
    "        group = group.sort_values(by=['start'])\n",
    "        while not group.empty:\n",
    "            row = group.iloc[0]\n",
    "            group = group.iloc[1:]\n",
    "            \n",
    "            overlapping_rows = group[(group['start'] < row['end']) & (group['end'] > row['start'])]\n",
    "            if not overlapping_rows.empty:\n",
    "                overlap_start = max(row['start'], overlapping_rows['start'].min())\n",
    "                overlap_end = min(row['end'], overlapping_rows['end'].max())\n",
    "\n",
    "                overlap_avail_qty = row['avail_qty'] + overlapping_rows['avail_qty'].sum()\n",
    "                overlap_nominal_power = row['nominal_power'] + overlapping_rows['nominal_power'].sum()\n",
    "\n",
    "                new_rows.append({\n",
    "                    'avail_qty': overlap_avail_qty,\n",
    "                    'nominal_power': overlap_nominal_power,\n",
    "                    'production_resource_name': name,\n",
    "                    'start': overlap_start,\n",
    "                    'end': overlap_end\n",
    "                })\n",
    "\n",
    "                if row['start'] < overlap_start:\n",
    "                    new_rows.append({\n",
    "                        'avail_qty': row['avail_qty'],\n",
    "                        'nominal_power': row['nominal_power'],\n",
    "                        'production_resource_name': name,\n",
    "                        'start': row['start'],\n",
    "                        'end': overlap_start\n",
    "                    })\n",
    "\n",
    "                if row['end'] > overlap_end:\n",
    "                    new_rows.append({\n",
    "                        'avail_qty': row['avail_qty'],\n",
    "                        'nominal_power': row['nominal_power'],\n",
    "                        'production_resource_name': name,\n",
    "                        'start': overlap_end,\n",
    "                        'end': row['end']\n",
    "                    })\n",
    "\n",
    "                group = group[~((group['start'] < row['end']) & (group['end'] > row['start']))]\n",
    "\n",
    "            else:\n",
    "                new_rows.append(row.to_dict())\n",
    "\n",
    "    # Create a new DataFrame from the new_rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Iterate over each file path in the reference CSV file\n",
    "for file_path in reference_df['Raw_Data_File_Path']:\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Process the dataframe to handle overlaps\n",
    "        df_result = handle_overlaps(df)\n",
    "        \n",
    "        # Save the result back to the CSV file\n",
    "        df_result.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"Processed and saved file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing file {file_path}: {e}\")\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91328d-f293-42b0-812d-de30f8a5a39d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6. Outage Factors Calculation \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Getting the Outage Factors for each time period.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.1. Raw Data Sources File Verification\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Verifying the column names of the raw data\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The web source used does not allways have the data from all countries e.g. the Cypres case. so this files are empty.\n",
    "    <br>\n",
    "Hence the same column names are going to be armonized to all the raw data files even those that are empty.\n",
    "    <br>\n",
    "This is needed to avoid errors in the next cells \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a805c2-a6c3-47f8-a286-57e0e6fac592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference data CSV file\n",
    "reference_df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Function to check if a CSV file is empty or has no headers\n",
    "def is_csv_empty_or_no_headers(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "        return True\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        if first_line == '':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "# Function to copy headers to an empty CSV file\n",
    "def copy_headers(source_file, target_file):\n",
    "    headers = pd.read_csv(source_file, nrows=0)\n",
    "    headers.to_csv(target_file, index=False)\n",
    "    print(f\"Copied headers from {source_file} to {target_file}\")\n",
    "\n",
    "# Clean the dataframe to remove NaN values in the 'Raw_Data_File_Path' column\n",
    "reference_df = reference_df.dropna(subset=['Raw_Data_File_Path'])\n",
    "\n",
    "# Iterate through each row in the reference dataframe to find a non-empty CSV file with headers\n",
    "non_empty_csv_with_headers = None\n",
    "for file_path in reference_df['Raw_Data_File_Path']:\n",
    "    file_path = str(file_path)\n",
    "    if os.path.exists(file_path) and not is_csv_empty_or_no_headers(file_path):\n",
    "        non_empty_csv_with_headers = file_path\n",
    "        break\n",
    "\n",
    "# Check if we have found a non-empty CSV file with headers\n",
    "if non_empty_csv_with_headers:\n",
    "    print(f\"Found non-empty CSV file with headers: {non_empty_csv_with_headers}\")\n",
    "    \n",
    "    # Iterate through each row in the reference dataframe\n",
    "    for file_path in reference_df['Raw_Data_File_Path']:\n",
    "        file_path = str(file_path)\n",
    "        if os.path.exists(file_path) and is_csv_empty_or_no_headers(file_path):\n",
    "            copy_headers(non_empty_csv_with_headers, file_path)\n",
    "else:\n",
    "    print(\"No non-empty CSV file with headers found to copy from.\")\n",
    "\n",
    "print(\"Header copy operation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6066d3-623f-47cc-8c74-7b421234beaa",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.2. Power Plants Data \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Getting the power plants data list.\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The total power capacity of each unit is required to calculate the Outage Factos.\n",
    "<br>\n",
    "The corresponding paths to each of the files where these informations is located is extrated to list called 'power_plants_data_file_path'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b08910-4304-4bcf-8784-1bae99eb084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plants_data_file_path = []  # Empty list to store power plant data file paths\n",
    "\n",
    "# Open the reference data file\n",
    "with open(reference_data_file_path, 'r') as reference_data_file:\n",
    "  reader = csv.DictReader(reference_data_file)\n",
    "\n",
    "  # Loop through each row in the reference data file\n",
    "  for row in reader:\n",
    "    dispa_set_code = row['Dispa-SET_Code']\n",
    "\n",
    "    # Construct the path to the power plant data file\n",
    "    power_plant_file_path = f\"{power_plants_data_folder_path}{dispa_set_code}/{data_year}.csv\"\n",
    "\n",
    "    # Append the path to the list\n",
    "    power_plants_data_file_path.append(power_plant_file_path)\n",
    "\n",
    "print(f\"Power plant data file paths extracted: {power_plants_data_file_path}\")\n",
    "\n",
    "power_plants_data_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203bacd-205c-4c84-8821-6a194da72ff4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Updating the reference file.\n",
    "<div/>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The reference_data_file_path define a csv file that contains all the paths of the csv files used to get the final formatted outages factors data.\n",
    "The same was created to manage the data in orderly way so the power_plants_data_file_path list is goint to be copiet to the reference file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30641632-a010-4f1d-adb5-edbe71df5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference data\n",
    "reference_df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Ensure the list and reference DataFrame have the same length\n",
    "if len(power_plants_data_file_path) != len(reference_df):\n",
    "    raise ValueError(\"The length of power_plants_data_file_path and the reference data must be the same\")\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "reference_df['Power_Plants_Data'] = power_plants_data_file_path\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "reference_df.to_csv(reference_data_file_path, index=False)\n",
    "\n",
    "print(\"Updated the reference data file with the new column 'Power_Plants_Data'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53113bf4-6c49-482a-a6ab-1f8175e7be8e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86714d-c711-472f-ba43-1a9a9c7b7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"universal_standard_time:                                   {universal_standard_time}\")\n",
    "print (f\"western_european_time:                                     {western_european_time}\")\n",
    "print (f\"central_european_time:                                     {central_european_time}\")\n",
    "print (f\"eastern_european_time:                                     {eastern_european_time}\")\n",
    "print (f\"reference_data_folder_path:                                {reference_data_folder_path}\")\n",
    "print (f\"reference_data_file_name:                                  {reference_data_file_name}\")\n",
    "print (f\"reference_data_file_path:                                  {reference_data_file_path}\")\n",
    "print (f\"outage_factors_folder_path:                                {outage_factors_folder_path}\")\n",
    "print(f\"outage_factors_raw_data_folder_path:                        {outage_factors_raw_data_folder_path}\")\n",
    "print(f\"power_plants_data_folder_path:                              {power_plants_data_folder_path}\")\n",
    "print(f\"power_plants_data_file_path:                                {power_plants_data_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ca67e-726c-4321-8a6c-db9eccca3127",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.3. Total Power Plant Capacity \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Copying the power plant capacity values.\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The corresponding total/nominal capacity of the units are copied from the already formatted power plants data file in the respective analized year.\n",
    "<br>\n",
    "These values later are used to calculate the outage factor over each time period of unavailability of the generation units.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e8bb1-04f5-4e9e-865f-862df0ad399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference data CSV file\n",
    "reference_df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Check if the number of entries in the reference file matches the length of the power plants data file paths list\n",
    "if len(reference_df) != len(power_plants_data_file_path):\n",
    "    raise ValueError(\"The number of entries in the reference file does not match the number of power plants data file paths\")\n",
    "\n",
    "# Iterate through each pair of Raw_Data_File_Path and power plants data file path\n",
    "for index, row in reference_df.iterrows():\n",
    "    file_path_1 = row['Raw_Data_File_Path']\n",
    "    file_path_2 = power_plants_data_file_path[index]\n",
    "\n",
    "    # Load the CSV files into dataframes\n",
    "    df1 = pd.read_csv(file_path_1)\n",
    "    df2 = pd.read_csv(file_path_2)\n",
    "\n",
    "    # Create a new column 'Total_Capacity' in df1 and initialize it with NaN\n",
    "    df1['Total_Capacity'] = pd.NA\n",
    "\n",
    "    # Iterate over each row in df2\n",
    "    for _, row in df2.iterrows():\n",
    "        unit_name = row['Unit']\n",
    "        power_capacity = row['PowerCapacity']\n",
    "\n",
    "        # Find matching rows in df1\n",
    "        matching_rows = df1[df1['production_resource_name'] == unit_name]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            # Update the 'Total_Capacity' in the matching rows\n",
    "            df1.loc[matching_rows.index, 'Total_Capacity'] = power_capacity\n",
    "\n",
    "    # Save the updated dataframe back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "\n",
    "    print(f\"Update completed for {file_path_1}.\")\n",
    "\n",
    "print(\"All updates completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb1090-0329-4106-be35-d874dd1e86b5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.4. Raw Data Files Outage Factors Calculation \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Getting the corresponding Outage Factors in a new column inside the raw data files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814796a-52b1-4e1a-9a4f-a772da6e12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference data CSV file\n",
    "reference_df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Iterate through each row in the reference dataframe\n",
    "for file_path in reference_df['Raw_Data_File_Path']:\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the target CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure the required columns exist\n",
    "        if 'avail_qty' in df.columns and 'nominal_power' in df.columns and 'Total_Capacity' in df.columns:\n",
    "            # Calculate the highest value between 'nominal_power' and 'Total_Capacity'\n",
    "            max_power_capacity = df[['nominal_power', 'Total_Capacity']].max(axis=1)\n",
    "\n",
    "            # Calculate the Outage Factor\n",
    "            df['Outage_Factor'] = df['avail_qty'] / max_power_capacity\n",
    "\n",
    "            # Save the updated dataframe back to the CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Processed file: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Missing required columns in file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c1d8489-3f51-4111-908d-c5678a74e3ff",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6. Outage Factors Files\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Getting the final formated files.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.1. Clean Data Files\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Creating the files where the clean outage data is going to be storaged.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The Outage Factors data has to be added using a determined time step as well, the same is going to be extracted from the Availability Factors already formated data from the corresponding year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fee9c-2be4-434f-82dc-2ab7178b9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Add new columns to the DataFrame with initial values (e.g., NaN or some default value)\n",
    "df['1h'] = pd.NA  # or use df['1h'] = 0 if you want to initialize with zeros\n",
    "df['30min'] = pd.NA  # or use df['30min'] = 0\n",
    "df['15min'] = pd.NA  # or use df['15min'] = 0\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df.to_csv(reference_data_file_path, index=False)\n",
    "\n",
    "# If you want to save it to a new file instead of overwriting the original one, you can use:\n",
    "# df.to_csv(\"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Updated_Reference_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ce352-1d33-4354-b48b-46da57d0329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference CSV file\n",
    "df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Get the list of Dispa-SET_Code values\n",
    "dispa_set_codes = df['Dispa-SET_Code'].unique()\n",
    "\n",
    "# Copy the folders and their contents\n",
    "for code in dispa_set_codes:\n",
    "    src_folder = os.path.join(availability_factors_data_folder_path, code)\n",
    "    dest_folder = os.path.join(outage_factors_folder_path, code)\n",
    "    \n",
    "    # Check if the source folder exists\n",
    "    if os.path.exists(src_folder):\n",
    "        # Copy the folder to the destination\n",
    "        shutil.copytree(src_folder, dest_folder, dirs_exist_ok=True)\n",
    "        print(f\"Copied folder: {code}\")\n",
    "        \n",
    "        # Remove unnecessary files in the copied folder\n",
    "        for filename in os.listdir(dest_folder):\n",
    "            file_path = os.path.join(dest_folder, filename)\n",
    "            if filename != f\"{data_year}.csv\":\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Source folder does not exist: {src_folder}\")\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfe549-b7e7-4c66-a3a2-62670a1c5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference CSV file\n",
    "df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Get the list of Dispa-SET_Code values\n",
    "dispa_set_codes = df['Dispa-SET_Code'].unique()\n",
    "\n",
    "# Columns to check\n",
    "columns = ['1h', '30min', '15min']\n",
    "\n",
    "# Process each Dispa-SET_Code folder\n",
    "for code in dispa_set_codes:\n",
    "    folder_path = os.path.join(outage_factors_folder_path, code)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        for col in columns:\n",
    "            subfolder_path = os.path.join(folder_path, col)\n",
    "            if os.path.exists(subfolder_path):\n",
    "                for filename in os.listdir(subfolder_path):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    if filename != f\"{data_year}.csv\":\n",
    "                        if os.path.isfile(file_path):\n",
    "                            os.remove(file_path)\n",
    "                            print(f\"Deleted file: {file_path}\")\n",
    "                    else:\n",
    "                        # Update the reference DataFrame with the path of the remaining CSV file\n",
    "                        df.loc[df['Dispa-SET_Code'] == code, col] = file_path\n",
    "    else:\n",
    "        print(f\"Folder does not exist: {folder_path}\")\n",
    "\n",
    "# Save the updated DataFrame back to the reference CSV file\n",
    "df.to_csv(reference_data_file_path, index=False)\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50238416-99d1-491a-8f33-67daaf758d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference data CSV file\n",
    "df_reference = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Define the columns to process\n",
    "columns_to_process = ['1h', '30min', '15min']\n",
    "\n",
    "# Iterate over the specified columns\n",
    "for column in columns_to_process:\n",
    "    # Iterate over each file path in the column\n",
    "    for file_path in df_reference[column].dropna():\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Keep only the first column\n",
    "        df = df.iloc[:, :1]\n",
    "        # Save the modified CSV file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Processed: {file_path}\")\n",
    "\n",
    "print(\"Task completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196d239-d230-4a40-8025-b96e095719fb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.2. Clean Data Files Headers\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Copying the power units to the Outage Factors clean data files.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The Outage Factors data files have to have all the power units name as headers to identify the corresponding Outage Factor belonging.\n",
    "<br>\n",
    "These names are copied from the already formated Power Plants files from the dispa-SET data base directory in the analized year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53688d-39f7-47c8-8548-baebd1446e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the reference data file\n",
    "#reference_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Reference_Data.csv\"\n",
    "\n",
    "# Load the reference data\n",
    "reference_df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Iterate over each row in the reference data\n",
    "for index, row in reference_df.iterrows():\n",
    "    power_plants_data_path = row['Power_Plants_Data']\n",
    "    \n",
    "    # Check if the power plants data file exists\n",
    "    if isinstance(power_plants_data_path, str) and os.path.exists(power_plants_data_path):\n",
    "        # Load the power plants data file\n",
    "        power_plants_df = pd.read_csv(power_plants_data_path)\n",
    "        \n",
    "        # Extract the 'Unit' column values\n",
    "        unit_values = power_plants_df['Unit'].tolist()\n",
    "        \n",
    "        # Iterate over the columns '1h', '30min', '15min'\n",
    "        for time_column in ['1h', '30min', '15min']:\n",
    "            dest_file_path = row[time_column]\n",
    "            \n",
    "            # Check if the destination file path is valid and exists\n",
    "            if isinstance(dest_file_path, str) and os.path.exists(dest_file_path):\n",
    "                # Read the destination file\n",
    "                dest_df = pd.read_csv(dest_file_path)\n",
    "                \n",
    "                # Create a new DataFrame with unit values as header\n",
    "                unit_df = pd.DataFrame(columns=unit_values)\n",
    "                \n",
    "                # Append the existing rows from the destination file to the new DataFrame\n",
    "                # and ignore the index to keep the original indexing\n",
    "                updated_df = pd.concat([dest_df, unit_df], axis=1)\n",
    "                \n",
    "                # Save the updated DataFrame back to the destination file\n",
    "                updated_df.to_csv(dest_file_path, index=False)\n",
    "                \n",
    "                print(f\"Updated file: {dest_file_path} with new headers from {power_plants_data_path}\")\n",
    "            else:\n",
    "                print(f\"Invalid or non-existent file path: {dest_file_path}, skipping.\")\n",
    "    else:\n",
    "        print(f\"Invalid or non-existent file path: {power_plants_data_path}, skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ed69a-9935-49a2-882d-e2de1a9c8f7d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Giving the corresponding first column header.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab92b02-a8c3-4598-8b9d-a1081d36cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Modifies a CSV file by duplicating the last row and shifting existing rows down.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Duplicate the last row and append it to the DataFrame\n",
    "    df = pd.concat([df, df.iloc[-1:]], ignore_index=True)\n",
    "\n",
    "    # Shift existing rows down in the first column\n",
    "    df.iloc[1:-1, 0] = df.iloc[:-2, 0].copy()\n",
    "\n",
    "    # Copy the header to the first row\n",
    "    df.iloc[0] = df.columns\n",
    "\n",
    "    # Rename the first column\n",
    "    df.columns = ['Unnamed: 0'] + df.columns[1:].tolist()\n",
    "\n",
    "    # Clear values in all rows except for the first column\n",
    "    df.iloc[:, 1:] = ''\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Processed file: {file_path}\")\n",
    "\n",
    "def process_files(reference_data_file_path):\n",
    "    \"\"\"\n",
    "    Processes CSV files based on information in the specified file.\n",
    "\n",
    "    Args:\n",
    "        reference_data_file_path: The path to the CSV file containing file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for column in ['1h', '30min', '15min']:\n",
    "            file_path = row[column]\n",
    "            if pd.notnull(file_path):\n",
    "                modify_csv_file(file_path)\n",
    "\n",
    "process_files(reference_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbeb736-4cca-4aae-911a-f40ef0119f60",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;color:skyblue\">\n",
    "6.3. Clean Outage Factors Data Files\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Copying all the already calculated outages factors to the clean files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b6bcd-6272-42df-bc4b-f54804a14a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_data = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Iterate over each row in the reference data\n",
    "for _, row in reference_data.iterrows():\n",
    "    raw_data_file_paths = [row['Raw_Data_File_Path'], row['1h'], row['30min'], row['15min']]\n",
    "    \n",
    "    # Skip if any field is empty\n",
    "    if any(pd.isnull(file_path) for file_path in raw_data_file_paths):\n",
    "        continue\n",
    "    \n",
    "    # Load file paths\n",
    "    file_path_1 = raw_data_file_paths[0]\n",
    "    file_paths_2 = raw_data_file_paths[1:]\n",
    "\n",
    "    # Load CSV files\n",
    "    df1 = pd.read_csv(file_path_1)\n",
    "    \n",
    "    # Convert 'start' and 'end' columns to datetime\n",
    "    df1['start'] = pd.to_datetime(df1['start'])\n",
    "    df1['end'] = pd.to_datetime(df1['end'])\n",
    "\n",
    "    # Iterate over file paths for file_path_2\n",
    "    for file_path_2 in file_paths_2:\n",
    "        df2 = pd.read_csv(file_path_2)\n",
    "        df2['Unnamed: 0'] = pd.to_datetime(df2['Unnamed: 0'])\n",
    "\n",
    "        # Iterate through each row in df1\n",
    "        for _, row in df1.iterrows():\n",
    "            start_time = row['start']\n",
    "            end_time = row['end']\n",
    "            outage_factor = row['Outage_Factor']\n",
    "            production_resource_name = row['production_resource_name']\n",
    "\n",
    "            # Filter df2 to rows within the time period\n",
    "            filtered_rows = df2[(df2['Unnamed: 0'] >= start_time) & (df2['Unnamed: 0'] <= end_time)]\n",
    "\n",
    "            # Iterate through filtered rows\n",
    "            for _, filtered_row in filtered_rows.iterrows():\n",
    "                # Paste the 'Outage_Factor' value into corresponding rows of df2\n",
    "                df2.at[filtered_row.name, production_resource_name] = outage_factor\n",
    "\n",
    "        # Save modified df2 back to CSV\n",
    "        df2.to_csv(file_path_2, index=False)\n",
    "\n",
    "print(\"Completed updating the CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05b43c-4c22-4200-a046-73b3e78086be",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Filling with a Outage Factor of '0' to all those power unit that do not have any unavailability in some time period.4\n",
    "<br>\n",
    "This is done to all the Dispa-SET countries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b305e-c429-4048-8da7-70a83919fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_data = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "# Iterate over each column in the reference data ('1h', '30min', '15min')\n",
    "for column in ['1h', '30min', '15min']:\n",
    "    # Iterate over each unique file path in the current column\n",
    "    for file_path in reference_data[column].dropna().unique():\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the CSV file has columns other than 'Unnamed: 0'\n",
    "        if len(df.columns) > 1:\n",
    "            # Fill empty fields of all columns except the first one with value '1'\n",
    "            df.iloc[:, 1:] = df.iloc[:, 1:].fillna(0)\n",
    "            \n",
    "            # Save the modified CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Filled empty fields in {file_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping {file_path} as it only has the 'Unnamed: 0' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6b223-e7cd-4f3f-8c2b-b34e59445c39",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;color:skyblue\">\n",
    "7. Copying Outage Factors Formatted Data Files\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;color:skyblue\">\n",
    "Moving the already clean and formatted data to the main Dispa-SET data base directory.\n",
    "     <br>\n",
    "    <div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;color:skyblue\">\n",
    "The data base directory has to be identified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1045bf9-8d33-4283-8a70-7a09cea3493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_4 = \"/Database/OutageFactors/\"\n",
    "\n",
    "# Construct the power_plants_raw_data_folder_path variable\n",
    "outages_factor_data_base_folder_path = dispaSET_unleash_folder_path + additional_path_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a9d38-d05b-4284-a8d9-f0df53d5186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reference_data(reference_data_file_path, outages_factor_data_base_folder_path):\n",
    "    \"\"\"\n",
    "    Processes the reference data and copies CSV files to corresponding subfolders, overwriting existing files.\n",
    "\n",
    "    Args:\n",
    "        reference_data_file_path: The path to the reference data CSV file.\n",
    "        outages_factor_data_base_folder_path: The path to the base folder for outage factors.\n",
    "    \"\"\"\n",
    "\n",
    "    reference_data = pd.read_csv(reference_data_file_path)\n",
    "\n",
    "    for index, row in reference_data.iterrows():\n",
    "        dispa_set_code = row['Dispa-SET_Code']\n",
    "        source_folder_path = os.path.join(outages_factor_data_base_folder_path, dispa_set_code)\n",
    "\n",
    "        if os.path.exists(source_folder_path) and os.path.isdir(source_folder_path):\n",
    "            # Ensure subfolders exist\n",
    "            for subfolder in ['1h', '30min', '15min']:\n",
    "                subfolder_path = os.path.join(source_folder_path, subfolder)\n",
    "                if not os.path.exists(subfolder_path):\n",
    "                    os.makedirs(subfolder_path)\n",
    "\n",
    "            # Copy CSV files, overwriting existing ones\n",
    "            for subfolder in ['1h', '30min', '15min']:\n",
    "                if not pd.isnull(row[subfolder]):\n",
    "                    source_file_path = row[subfolder]\n",
    "                    destination_file_path = os.path.join(source_folder_path, subfolder, os.path.basename(source_file_path))\n",
    "                    shutil.copy2(source_file_path, destination_file_path)\n",
    "                    print(f\"Copied {os.path.basename(source_file_path)} to {destination_file_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping empty or non-existent folder: {source_folder_path}\")\n",
    "\n",
    "process_reference_data(reference_data_file_path, outages_factor_data_base_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e04e06-7b0a-450f-8b04-d9412dac29cb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "7. Outages Factors Folder Back Up\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Once all the formating process was done the Outages Factors Folder is restored to its defoult state.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e182b9-512a-4e07-ae01-95951ca85e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(outage_factors_folder_path):\n",
    "    shutil.rmtree(outage_factors_folder_path)  # Remove the current directory\n",
    "shutil.copytree(backup_folder_path, outage_factors_folder_path)\n",
    "\n",
    "print(f\"Directory restored to original state from {backup_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ebc33-a9b6-45d8-b4e1-4e15c93b39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(backup_folder_path)\n",
    "print(f\"Backup folder {backup_folder_path} deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c264d6-904c-46d2-a3b4-c85d9f0b5413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
